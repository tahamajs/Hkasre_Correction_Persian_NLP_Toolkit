{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0_30xV22oa6"
   },
   "source": [
    "\n",
    "# Natural Language Processing Homework 2  \n",
    "\n",
    "sharif University of technology\n",
    "\n",
    "\n",
    "### 1. Proper Correction of Ezafe Usage\n",
    "\n",
    "**Objective:**  \n",
    "Develop a system that identifies and corrects common errors in the usage of the Persian Ezafe (the connecting vowel “ِ”) and the attached letter “ه.” These errors are prevalent in everyday writing and can lead to mispronunciations or misunderstandings.\n",
    "\n",
    "**Requirements:**  \n",
    "- The system must accept an input Persian text.\n",
    "- Identify the errors where the Ezafe is missing or misused (e.g., incorrect use of “ه” instead of “هٔ” or missing \"ـه\").\n",
    "- For each detected issue, return:\n",
    "  - The corrected full sentence.\n",
    "  - The **range** (start and end indices) of each erroneous word.\n",
    "  - The **corrected form** of each erroneous word.\n",
    "\n",
    "---\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "| Input (Persian Text) | Output (JSON Response) |\n",
    "|----------------------|------------------------|\n",
    "| حاله من خیلی خوب.    | ```json<br>{<br>  \"correct\": \"حال من خیلی خوبه.\",<br>  \"حاله\": [0, 4],<br>  \"خوب\": [13, 16]<br>}<br>``` |\n",
    "| می‌خواهم پرنده‌ از شاخه، روی بامه خانه بپر. | ```json<br>{<br>  \"correct\": \"می‌خواهم پرنده از شاخه، روی بام خانه بپرد.\",<br>  \"خانه\": [27, 31],<br>  \"بپر\": [37, 40]<br>}<br>``` |\n",
    "| دیروز آن مرده قوی مرد. | ```json<br>{<br>  \"correct\": \"دیروز آن مرد قوی مرد.\",<br>  \"مرده\": [9, 13]<br>}<br>``` |\n",
    "\n",
    "These examples show how the system:\n",
    "- Detects incorrect forms like “حاله” and corrects them to “حالِ”\n",
    "- Restores missing Ezafe (e.g., “بامه” → “بامِ”)\n",
    "- Fixes improper verb endings or morphological issues.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Word and Sentence Tokenization\n",
    "\n",
    "**Objective:**  \n",
    "Preprocessing in natural language processing starts with dividing the text into sentences and words. You are to create tokenizers that segment an input Persian text into its component sentences and then further into individual words.\n",
    "\n",
    "**Requirements:**  \n",
    "- Your module should take a single Persian text string as input.\n",
    "- **Output:**  \n",
    "  - A list of sentences extracted from the text.  \n",
    "  - A nested list (or similar structure) where each sublist contains the tokens (words) of the corresponding sentence.\n",
    "\n",
    "**Example:**  \n",
    "- **Input:**  \n",
    "  ```\n",
    "  امروز هوا خوب است. فردا شاید بارانی باشد!\n",
    "  ```\n",
    "- **Output:**  \n",
    "  ```json\n",
    "  {\n",
    "    \"sentences\": [\n",
    "      \"امروز هوا خوب است.\",\n",
    "      \"فردا شاید بارانی باشد!\"\n",
    "    ],\n",
    "    \"tokens\": [\n",
    "      [\"امروز\", \"هوا\", \"خوب\", \"است\", \".\"],\n",
    "      [\"فردا\", \"شاید\", \"بارانی\", \"باشد\", \"!\"]\n",
    "    ]\n",
    "  }\n",
    "  ```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Creating a Text Normalizer\n",
    "\n",
    "**Objective:**  \n",
    "Implement a normalization module that standardizes Persian text. Normalization can include removing extra spaces, converting variant forms of characters (for example, replacing Arabic variants with Persian ones), and transforming informal spellings into their formal equivalents.\n",
    "\n",
    "**Requirements:**  \n",
    "- Build a module (or extend an existing one) to normalize text.\n",
    "- Demonstrate the transformation on sample texts by showing before-and-after examples.\n",
    "- Include multiple test cases that highlight the strengths and potential weaknesses of your normalization approach.\n",
    "\n",
    "**Example:**  \n",
    "- **Input:**  \n",
    "  ```\n",
    "  میخوااام اینو درست کنم!!!\n",
    "  ```\n",
    "- **Output:**  \n",
    "  ```\n",
    "  می‌خواهم این را درست کنم!\n",
    "  ```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Detection of Illegal Words\n",
    "\n",
    "**Objective:**  \n",
    "Some automated systems (bots) are tasked with detecting “illegal” or forbidden words in text. However, if extra characters—such as non-Persian letters, numbers, or special characters—are inserted between the letters, these systems may fail. Your task is to create a system that can robustly detect these words despite such modifications.\n",
    "\n",
    "**Function Signature:**\n",
    "```python\n",
    "run(input: str, illegal_words: list)\n",
    "```\n",
    "\n",
    "**Requirements:**  \n",
    "- **Input:**  \n",
    "  - A Persian text string.  \n",
    "  - A list of illegal words to search for.\n",
    "- **Output:**  \n",
    "  - A list or dictionary indicating which illegal words were detected, along with the positions (or ranges) in the text where they were found.  \n",
    "  - Your solution must ignore extraneous characters (e.g., “#”, “…”, spaces, etc.) inserted between the letters of a potentially illegal word.\n",
    "\n",
    "---\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "| **Input**                                   | **Illegal Words**         | **Output (JSON)**                                              |\n",
    "|--------------------------------------------|---------------------------|-----------------------------------------------------------------|\n",
    "| `این گفت:#ر ی# ازخوشم`                     | `[\"تفنگ\"]`                | ```json<br>{<br>  \"تفنگ\": [4, 14]<br>}<br>```                  |\n",
    "| `با ما رفتم، به #گ...غ غذا نخورد.`         | `[\"قاشق\", \"چنگال\"]`       | ```json<br>{<br>  \"قاشق\": [3, 10],<br>  \"چنگال\": [14, 23]<br>}<br>``` |\n",
    "\n",
    "In these examples, the system successfully detects the words `\"تفنگ\"`, `\"قاشق\"`, and `\"چنگال\"` despite the presence of additional characters interspersed between the Persian letters.\n",
    "\n",
    "---\n",
    "\n",
    "**Additional Example**  \n",
    "Consider the following input, which has punctuation between each letter:\n",
    "\n",
    "- **Input:**  \n",
    "  ```\n",
    "  م..ی..خ..ر..ی\n",
    "  ```  \n",
    "- **Illegal Words:**  \n",
    "  ```\n",
    "  [\"میخری\"]\n",
    "  ```  \n",
    "- **Output (illustrative):**  \n",
    "  ```json\n",
    "  {\n",
    "    \"detections\": [\n",
    "      { \"range\": [0, 9], \"matched_word\": \"میخری\" }\n",
    "    ]\n",
    "  }\n",
    "  ```\n",
    "\n",
    "Here, the system identifies that `\"م..ی..خ..ر..ی\"` effectively matches the illegal word `\"میخری\"`, ignoring the extra dots in between.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Extraction of Conjunctions and Subsentences\n",
    "\n",
    "**Objective:**  \n",
    "Persian sentences are often compound and may be composed of multiple clauses connected by conjunctions (such as \"و\", \"یا\", \"اما\", etc.). Your task is to:\n",
    "\n",
    "1. Split the text into its constituent subsentences.  \n",
    "2. Extract the conjunction words used to connect these clauses.  \n",
    "3. Identify the type of each conjunction (e.g., coordinating, subordinating, adversative, dual) and record the position (index range) of each occurrence.  \n",
    "\n",
    "If a conjunction appears more than once, differentiate each occurrence (e.g., “و1,” “و2,” etc.).\n",
    "\n",
    "---\n",
    "\n",
    "**Requirements:**  \n",
    "- **Input:**  \n",
    "  - A single Persian text string.  \n",
    "- **Output:**  \n",
    "  1. A list (or array) of subsentences.  \n",
    "  2. A data structure (e.g., a list or dictionary) that includes each extracted conjunction, its type, and its location (start and end indices) within the text.\n",
    "\n",
    "---\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "| **Input**                                                                                                 | **Output (JSON)**                                                                                                                                                                                              |\n",
    "|-----------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| امروز در خبرها آمده است که هوای شهر نیمه ابری است. <br> او درس خواند اما نمره‌اش خوب نشد و کلّی یاد گرفت. | ```json<br>{<br>  \"splits\": [<br>    \"امروز در خبرها آمده است که هوای شهر نیمه ابری است.\",<br>    \"او درس خواند\",<br>    \"اما نمره‌اش خوب نشد\",<br>    \"و کلّی یاد گرفت.\"<br>  ],<br>  \"conjunctions\": [<br>    {<br>      \"word\": \"اما\",<br>      \"type\": \"adversative\",<br>      \"range\": [26, 28]<br>    },<br>    {<br>      \"word\": \"و\",<br>      \"type\": \"coordinating\",<br>      \"range\": [45, 45]<br>    }<br>  ]<br>}<br>``` |\n",
    "| رفت به دوستش گفت بیا با هم بازی کنیم.                                                                     | ```json<br>{<br>  \"splits\": [<br>    \"رفت به دوستش گفت\",<br>    \"بیا با هم بازی کنیم.\"<br>  ],<br>  \"conjunctions\": []<br>}<br>```                                                                                 |\n",
    "| هم فال بود هم تماشا.                                                                                      | ```json<br>{<br>  \"splits\": [<br>    \"هم فال بود\",<br>    \"هم تماشا.\"<br>  ],<br>  \"conjunctions\": [<br>    {<br>      \"word\": \"هم\",<br>      \"type\": \"dual\",<br>      \"range\": [0, 1]<br>    },<br>    {<br>      \"word\": \"هم\",<br>      \"type\": \"dual\",<br>      \"range\": [9, 10]<br>    }<br>  ]<br>}<br>``` |\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation of the Examples\n",
    "\n",
    "1. **Multiple Conjunctions in a Single Paragraph**  \n",
    "   - **Input:**  \n",
    "     ```\n",
    "     امروز در خبرها آمده است که هوای شهر نیمه ابری است. او درس خواند اما نمره‌اش خوب نشد و کلّی یاد گرفت.\n",
    "     ```\n",
    "   - **Output:**  \n",
    "     - **splits:** Each sentence or clause is separated based on punctuation or recognized conjunction words.  \n",
    "     - **conjunctions:** The words “اما” and “و” are identified, along with their indices in the text. Their types are labeled as “adversative” and “coordinating,” respectively.\n",
    "\n",
    "2. **No Conjunction Present**  \n",
    "   - **Input:**  \n",
    "     ```\n",
    "     رفت به دوستش گفت بیا با هم بازی کنیم.\n",
    "     ```\n",
    "   - **Output:**  \n",
    "     - **splits:** Two main clauses are identified.  \n",
    "     - **conjunctions:** An empty list, since no explicit conjunction words (“و,” “یا,” “اما,” etc.) are found.\n",
    "\n",
    "3. **Repeated Conjunction**  \n",
    "   - **Input:**  \n",
    "     ```\n",
    "     هم فال بود هم تماشا.\n",
    "     ```\n",
    "   - **Output:**  \n",
    "     - **splits:** The text is split into two clauses: “هم فال بود” and “هم تماشا.”  \n",
    "     - **conjunctions:** The word “هم” appears twice and is considered a dual conjunction. The positions (ranges) for each occurrence are recorded separately.\n",
    "\n",
    "---\n",
    "\n",
    "### Implementation Tips\n",
    "\n",
    "- You can use a combination of **regular expressions** and **string processing** to detect conjunctions.\n",
    "- Decide on a set of known conjunctions (e.g., “و,” “یا,” “اما,” “هم,” “ولی,” etc.) and classify them as coordinating, subordinating, adversative, or dual.\n",
    "- Track each conjunction’s **start** and **end** indices in the original string to fill out the `\"conjunctions\"` data structure.\n",
    "- For splitting into subsentences, look for:\n",
    "  1. **Conjunctions**  \n",
    "  2. **Punctuation** (e.g., “.”, “!”, “؟”)  \n",
    "  3. **Possible multi-sentence boundaries** (some Persian texts omit explicit punctuation)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Extraction of Food Order Features from a Message\n",
    "\n",
    "**Objective:**  \n",
    "Develop a system that analyzes a text message to extract features related to a food order. The message may include details such as the type of food, size or quantity, and any special instructions or extras.\n",
    "\n",
    "**Requirements:**  \n",
    "1. **Input Text Analysis:**  \n",
    "   - Identify **food items** (e.g., “پیتزا,” “کباب,” “سبزی پلو,” “ماهی,” “سالاد,” etc.).  \n",
    "   - Detect **sizes** or **quantities** (e.g., “یک,” “دوتا,” “بزرگ,” “کوچک,” etc.).  \n",
    "   - Extract **extras** or **special instructions** (e.g., “سس اضافه,” “نوشابه,” “خیلی خوب شسته شده,” “چرخ کرده,” etc.).  \n",
    "2. **Output:**  \n",
    "   - A structured representation (such as JSON) that clearly specifies the details of the order:  \n",
    "     - **food**: List of all mentioned dishes or items.  \n",
    "     - **quantity/size**: If provided (single or multiple).  \n",
    "     - **extras/notes**: Any additional preferences or instructions.\n",
    "\n",
    "---\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "1. **Example 1**  \n",
    "   - **Input:**  \n",
    "     ```\n",
    "     لطفاً یک پیتزای بزرگ با سس اضافه بیار\n",
    "     ```\n",
    "   - **Output:**  \n",
    "     ```json\n",
    "     {\n",
    "       \"food\": \"پیتزا\",\n",
    "       \"size\": \"بزرگ\",\n",
    "       \"extras\": \"سس اضافه\"\n",
    "     }\n",
    "     ```\n",
    "\n",
    "2. **Example 2**  \n",
    "   - **Input:**  \n",
    "     ```\n",
    "     من دوتا کباب میخوام، یکی معمولی و یکی تند.\n",
    "     ```\n",
    "   - **Output:**  \n",
    "     ```json\n",
    "     {\n",
    "       \"food\": \"کباب\",\n",
    "       \"quantity\": 2,\n",
    "       \"types\": [\"معمولی\", \"تند\"]\n",
    "     }\n",
    "     ```\n",
    "\n",
    "3. **Example 3 (from Screenshot)**  \n",
    "   - **Input:**  \n",
    "     ```\n",
    "     بی زحمت یک سبزی پلو با ماهی و اگر آب دریا خوب بوده، لطفا قزل آلا بگذار. اگر سبزی پاک نشده باشد.\n",
    "     ```\n",
    "   - **Output:**  \n",
    "     ```json\n",
    "     {\n",
    "       \"food\": [\n",
    "         \"سبزی پلو با ماهی\",\n",
    "         \"قزل آلا\"\n",
    "       ],\n",
    "       \"extras\": [\n",
    "         \"سبزی پاک نشده\"\n",
    "       ]\n",
    "     }\n",
    "     ```\n",
    "   In this text, the system detects two main dishes (“سبزی پلو با ماهی” and “قزل آلا”) and an additional note or instruction regarding “سبزی پاک نشده.”\n",
    "\n",
    "4. **Example 4 (from Screenshot)**  \n",
    "   - **Input:**  \n",
    "     ```\n",
    "     سلام یک پیتزای قارچ و گوشت میخواستم لطفا خیلی خوب شسته شده باشد و گوشت چرخ کرده باشد.\n",
    "     ```\n",
    "   - **Output:**  \n",
    "     ```json\n",
    "     {\n",
    "       \"food\": [\n",
    "         \"پیتزای قارچ و گوشت\"\n",
    "       ],\n",
    "       \"extras\": [\n",
    "         \"خیلی خوب شسته شده\",\n",
    "         \"گوشت چرخ کرده\"\n",
    "       ]\n",
    "     }\n",
    "     ```\n",
    "   Here, the primary food item is “پیتزای قارچ و گوشت,” and there are two separate instructions or notes: “خیلی خوب شسته شده” and “گوشت چرخ کرده.”\n",
    "\n",
    "---\n",
    "\n",
    "### Implementation Suggestions\n",
    "\n",
    "- **Tokenization & Keyword Detection**:  \n",
    "  Use either a custom tokenizer or an existing Persian tokenizer to split the text into meaningful tokens. Then, look for known food keywords (e.g., “پیتزا,” “ماهی,” etc.) and special instruction phrases.\n",
    "\n",
    "- **Regular Expressions**:  \n",
    "  - Can help capture patterns like “سس اضافه,” “خیلی خوب شسته شده,” or any numeric/quantifier phrase (“یک,” “دوتا,” etc.).  \n",
    "  - Identify patterns that might indicate multiple items, such as “و” (and) or punctuation.\n",
    "\n",
    "- **Parsing Logic**:  \n",
    "  - Once a food item is found, note it in the `\"food\"` field.  \n",
    "  - If a size/quantity keyword is encountered, store it in `\"size\"` or `\"quantity\"`.  \n",
    "  - Any leftover descriptive or directive phrases go into `\"extras\"` or `\"notes\"`.\n",
    "\n",
    "  ```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Detection of a Sequence of Actions\n",
    "\n",
    "**Objective:**  \n",
    "Many texts describe a series of actions or instructions that need to be performed in order. Your task is to:\n",
    "\n",
    "1. Detect sequencing keywords such as **\"ابتدا\"** (first), **\"سپس\"** (then), **\"در نهایت\"** (finally), and similar phrases that imply sequential actions.  \n",
    "2. Split the text into individual steps that reflect the sequence of actions.  \n",
    "3. **Output** a structured list or dictionary containing the ordered steps.\n",
    "\n",
    "---\n",
    "\n",
    "**Implementation Hints:**\n",
    "\n",
    "- **Parsing Sequential Cues**: Look for words like “ابتدا,” “بعد,” “سپس,” “در نهایت,” or any similar adverbial phrase indicating order.  \n",
    "- **Segmentation**: Use these keywords and punctuation to split the text into discrete steps.  \n",
    "- **Normalization**: You may need to normalize or simplify certain phrases (e.g., “می‌توانیم سمنو اضافه کنیم” → “سمنو اضافه کن”) to store them consistently in your output.\n",
    "\n",
    "---\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "#### Example 1\n",
    "\n",
    "| **Input**                                                                                                                              | **Output (JSON)**                                                                                                              |\n",
    "|----------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------|\n",
    "| برای پخت غذای عید لازم است سیر سرخ کنیم ولی قبلش باید سیب بخوریم، در گام سوم باید سمنو را هم اضافه کنیم و بعد سیرها را با آن‌ها ترکیب می‌کنیم. | ```json<br>{<br>  \"goal\": [<br>    \"پخت غذای عید\",<br>    \"سرخ سیر\",<br>    \"سیب بخور\",<br>    \"سمنو اضافه کن\",<br>    \"سیرها ترکیب کن\"<br>  ]<br>}<br>``` |\n",
    "\n",
    "Explanation:\n",
    "1. **پخت غذای عید** is identified as the first overall goal or action.\n",
    "2. **سرخ سیر** is the second step.\n",
    "3. **سیب بخور** is inferred from “باید سیب بخوریم.”\n",
    "4. **سمنو اضافه کن** is extracted from “سمنو را هم اضافه کنیم.”\n",
    "5. **سیرها ترکیب کن** is derived from “سیرها را با آن‌ها ترکیب می‌کنیم.”\n",
    "\n",
    "---\n",
    "\n",
    "#### Example 2\n",
    "\n",
    "| **Input**                                                                                              | **Output (JSON)**                                                                                                  |\n",
    "|--------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------|\n",
    "| جهت انجام تکالیف، ابتدا پشت میز بنشینید، سپس روی آن فکر کنید و درنهایت زمان کافی برای حل سوالات بگذارید. | ```json<br>{<br>  \"goal\": [<br>    \"انجام تکالیف\",<br>    \"پشت میز بنشین\",<br>    \"روی آن فکر کن\",<br>    \"زمان کافی برای حل سوالات بگذار\"<br>  ]<br>}<br>``` |\n",
    "\n",
    "Explanation:\n",
    "1. **انجام تکالیف** is the main goal.\n",
    "2. **پشت میز بنشین** is identified from “ابتدا پشت میز بنشینید.”\n",
    "3. **روی آن فکر کن** corresponds to “روی آن فکر کنید.”\n",
    "4. **زمان کافی برای حل سوالات بگذار** is derived from “در نهایت زمان کافی برای حل سوالات بگذارید.”\n",
    "\n",
    "---\n",
    "\n",
    "#### Example 3 (Basic)\n",
    "\n",
    "- **Input:**  \n",
    "  ```\n",
    "  ابتدا باید آرد را آماده کنم، بعد خمیر درست کنم و در نهایت آن را بپزم.\n",
    "  ```\n",
    "- **Output:**  \n",
    "  ```json\n",
    "  {\n",
    "    \"actions\": [\n",
    "      \"آرد را آماده کن\",\n",
    "      \"خمیر درست کن\",\n",
    "      \"آن را بپز\"\n",
    "    ]\n",
    "  }\n",
    "  ```\n",
    "\n",
    "In this simpler example, the keywords **\"ابتدا\"**, **\"بعد\"**, and **\"در نهایت\"** guide the segmentation of the text into three clear steps.\n",
    "\n",
    "---\n",
    "\n",
    "### Final Notes\n",
    "\n",
    "- Ensure that each **step** or **goal** is stored in a concise, imperative form (e.g., “بخور,” “بپز,” “بنشین”) to maintain consistency.  \n",
    "- You may want to handle **edge cases** where sequential keywords are implied but not explicitly stated.  \n",
    "- Combining **regular expressions**, **tokenization**, and a **keyword dictionary** can be very effective for this task.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Extraction of English Words Written in Persian\n",
    "\n",
    "**Objective:**  \n",
    "Occasionally, English words appear within Persian text, sometimes written phonetically in Persian script. Your task is to identify these words and, if possible, provide their standard English equivalents.\n",
    "\n",
    "**Requirements:**  \n",
    "1. **Analyze** an input Persian text for tokens that are likely English words.  \n",
    "2. **Output:**  \n",
    "   - A list of detected words, along with their positions (start and end indices) in the text.  \n",
    "   - When possible, include a mapping to the standard English word.\n",
    "\n",
    "---\n",
    "\n",
    "**Implementation Tips:**\n",
    "\n",
    "- **Tokenization:** Use a tokenizer (custom or existing) to split the text into words.  \n",
    "- **Heuristic or Dictionary Matching:**  \n",
    "  - Create a dictionary or heuristic rules for identifying potential English-origin words (e.g., words that contain certain phonetic sequences or that are known loanwords).  \n",
    "  - Alternatively, use a machine-learning or statistical approach if you have a labeled dataset of transliterated English words in Persian.\n",
    "- **Mapping to English:**  \n",
    "  - Maintain a lookup table of common transliterated words (e.g., “کامپیوتر” → “computer,” “سیستم” → “system,” “هارد” → “hard,” etc.).  \n",
    "  - For words not in your lookup, you can leave the `standard_english` field empty or attempt a best-guess transliteration.\n",
    "\n",
    "---\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "#### Example 1\n",
    "\n",
    "- **Input:**  \n",
    "  ```\n",
    "  امروز یک کار خیلی هاردی داشتیم. ولی تو کانتریبیوشن خویی داشتی. تنکس.\n",
    "  ```\n",
    "- **Output (JSON):**  \n",
    "  ```json\n",
    "  {\n",
    "    \"english_words\": [\n",
    "      {\n",
    "        \"word_in_persian\": \"هارد\",\n",
    "        \"range\": [18, 22],\n",
    "        \"standard_english\": \"hard\"\n",
    "      },\n",
    "      {\n",
    "        \"word_in_persian\": \"کانتریبیوشن\",\n",
    "        \"range\": [40, 51],\n",
    "        \"standard_english\": \"contribution\"\n",
    "      },\n",
    "      {\n",
    "        \"word_in_persian\": \"تنکس\",\n",
    "        \"range\": [64, 68],\n",
    "        \"standard_english\": \"thanks\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "  ```\n",
    "\n",
    "Explanation:  \n",
    "- The system detects “هارد,” “کانتریبیوشن,” and “تنکس” as transliterated English words.  \n",
    "- Each entry includes its position (index range) in the text and a possible standard English equivalent.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example 2\n",
    "\n",
    "- **Input:**  \n",
    "  ```\n",
    "  سیستم کامپیوتر خراب شده‌است.\n",
    "  ```\n",
    "- **Output (JSON):**  \n",
    "  ```json\n",
    "  {\n",
    "    \"english_words\": [\n",
    "      {\n",
    "        \"word_in_persian\": \"سیستم\",\n",
    "        \"range\": [0, 5],\n",
    "        \"standard_english\": \"system\"\n",
    "      },\n",
    "      {\n",
    "        \"word_in_persian\": \"کامپیوتر\",\n",
    "        \"range\": [6, 14],\n",
    "        \"standard_english\": \"computer\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "  ```\n",
    "\n",
    "Explanation:  \n",
    "- The words “سیستم” and “کامپیوتر” are recognized as likely English-origin terms, mapped here to “system” and “computer,” respectively.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Position Tracking:** Always record the exact indices (start and end) for each detected English-origin word in the text.  \n",
    "- **Flexible Matching:** Some words may have multiple acceptable transliterations (e.g., “سیستم” could sometimes appear as “سیستوم”). Decide on a consistent approach for mapping them back to English.  \n",
    "- **Coverage vs. Precision:** Consider balancing the breadth of your dictionary (catching as many English words as possible) with the precision of your detection (minimizing false positives).\n",
    "\n",
    "By following these steps, your system will accurately detect and map English words embedded in Persian text, providing a clear and structured output.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Notes\n",
    "\n",
    "- Each task may be implemented either by creating a custom module or by extending existing open-source libraries.\n",
    "- It is recommended that you provide comprehensive test cases and examples to demonstrate the robustness of your solutions.\n",
    "- Ensure your final submission adheres to the guidelines regarding code execution, file uploads, and documentation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xh7Rr-AuIANQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RAmvwTvGxwO"
   },
   "source": [
    "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir='rtl' align='justify'>\n",
    "#    **خطای هه کسره**\n",
    "در نوشتار فارسی، خطای \"ه‌کسره\" هنگامی به وجود می‌آید  که نشان کسره به درستی استفاده نشود.\n",
    "با اینکه صدای \"e\" در زبان فارسی دارای چندین نوع تکواژ است، اما برای نمایش آن در نوشتار دو نماد تکواژی وجود دارد. در مواقعی که به جای کسره (ـــِ) از \"ه/ـه\" استفاده شود یا برعکس، خطای گرامری هکسره به وجود می‌آید. در این تمرین، سرویسی را پیاده‌سازی کرده‌ایم که با دریافت یک متن فارسی، خطاهای «ه‌هکسره» آن را تشخیص داده و متن تصحیح شده را در پاسخ بر می‌گرداند. در ادامه گزارش، جزئیات پیاده‌سازی تمرین، و شیوه بکاررفته برای تشخیص خطای ه‌کسره شرح داده شده‌است.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzBffAvSGxwO"
   },
   "source": [
    "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir='rtl' align='justify'>\n",
    "### **نصب پکیج‌ها و ابزارهای مورد نیاز**\n",
    "\n",
    "کتاب‌خانه‌های اصلی مورد استفاده در این تمرین، کتاب‌خانه‌های هضم و دادماتولز بوده‌اند. کتاب‌خانه هضم برای POS Tagging و دادماتولز برای بررسی شباهت کلمات استفاده شده‌است."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "U_ECGenxGxwO",
    "outputId": "ee2ae9c8-2da0-4278-e6bd-53ae8a3aee0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.4-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import sklearn\n",
    "except:\n",
    "    %pip install -U scikit-learn numpy\n",
    "\n",
    "try:\n",
    "    import hazm\n",
    "except:\n",
    "    %pip install hazm\n",
    "\n",
    "try:\n",
    "    import dadmatools\n",
    "except:\n",
    "    %pip install dadmatools\n",
    "\n",
    "try:\n",
    "    import fasttext\n",
    "except:\n",
    "    %pip install fasttext\n",
    "\n",
    "try:\n",
    "    import wapiti\n",
    "except:\n",
    "    %pip install wapiti\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yku9-tEzGxwP"
   },
   "source": [
    "## **Code Synchronization with the parsi-io Library**\n",
    "\n",
    "In implementing the code for this exercise, an effort was made to align the service's implementation with the design pattern used in the parsi-io library.  \n",
    "The `HeKasraExtractor` class is the main component of this service, and it has been developed using a framework compatible with the aforementioned library. However, due to the exercise upload deadline, there wasn’t an opportunity to submit a pull request to the library’s repository. Once the contribution guidelines and rules are fully met in that project’s repository, we will attempt to submit a pull request to the parsi-io repository in the near future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzJeC323GxwQ"
   },
   "source": [
    "# **Identifying and Correcting the Heh-Kasra Errors**\n",
    "\n",
    "In Persian writing, the *Heh-Kasra* error occurs according to specific patterns. Generally, these patterns are fairly rule-based, and despite some exceptions, *Heh-Kasra* mistakes can be categorized into a few common types.  \n",
    "This [blog post](https://blog.irandargah.com/%D8%BA%D9%84%D8%B7%E2%80%8C%D9%87%D8%A7%DB%8C-%D9%86%DA%AF%D8%A7%D8%B1%D8%B4%DB%8C-%D9%88-%D8%A7%D9%85%D9%84%D8%A7%DB%8C%DB%8C%D8%8C-%D9%82%D8%A7%D8%AA%D9%84-%D8%A7%D8%B9%D8%AA%D8%A8%D8%A7%D8%B1/) offers a brief and useful overview of the various types of Heh-Kasra errors in Persian.  \n",
    "In this exercise, the implementation for detecting Heh-Kasra errors has also been based on such resources. Below is a brief explanation of the common patterns associated with this spelling mistake.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Patterns of the Heh-Kasra Mistake\n",
    "\n",
    "1. **Adjective-Noun or Ezafe Constructions**  \n",
    "   In descriptive or possessive constructions, an Ezafe (ــِ) must be used between the two relevant words. Using \"ه\" or \"ـه\" instead in these contexts is incorrect.  \n",
    "   * **Exception: When the morpheme “ه” is part of the word**  \n",
    "   There are cases where the letter “ه/ـه” is actually an integral part of the word, appearing at the end and known as a *silent Heh*. In such cases, the “ه/ـه” should not be removed, and replacing it with a Kasra is inappropriate.\n",
    "\n",
    "2. **The morpheme “ه” as a definiteness marker**  \n",
    "   In some cases, \"ه\" is added to the end of words to make them definite (i.e., to refer to a specific person or object known to the speaker). Using a Kasra (ـِ) instead of this “ه” is entirely incorrect.\n",
    "\n",
    "3. **The morpheme “ه” as a substitute for a verb**  \n",
    "   In spoken Persian, sometimes the sound “e” is used instead of the verb *“ast”* (is) or *“hast”* (exists). In these cases, “ه” should be used, not a Kasra.  \n",
    "   Also, for third-person verbs in colloquial speech, instead of ending them with “ـَد”, the sound “e” is sometimes used. In such cases too, the correct usage is “ه” rather than a Kasra.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEZx7BEjGxwQ"
   },
   "source": [
    "Of course! Here's the English translation of your text:\n",
    "\n",
    "---\n",
    "\n",
    "## **Implementation of the Heh-Kasra Error Detection System**\n",
    "\n",
    "---\n",
    "\n",
    "### The `HeKasraCorrection` Class\n",
    "\n",
    "An object of this class holds the processed text (along with the original raw text). The methods of this object are called by the components of the Heh-Kasra detection pipeline.  \n",
    "If a method in the pipeline detects a Heh-Kasra error, it reports the error as a potential correction using the `vote_for_correction` function. The `order` argument in this function indicates the priority of the correction.\n",
    "\n",
    "---\n",
    "\n",
    "The `veto_correction` function in this class, when triggered by a component in the pipeline, vetoes the corrections suggested by previous components.  \n",
    "For example, in the phrase “خانه زیبا” (*beautiful house*), an early function in the pipeline might wrongly detect a Heh-Kasra error in this descriptive compound. But a later function recognizes that the “ه” is part of the word “خانه” (*house*), meaning it's not an error. Therefore, the earlier error detection is vetoed.\n",
    "\n",
    "---\n",
    "\n",
    "Finally, the `finalize` function is called at the end, after all the modules in the pipeline have cast their votes regarding the Heh-Kasra errors in the text. This function gathers the errors, applies them based on their priority, generates the corrected text, and identifies the errors and their corresponding ranges in the original input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUJIKyhzGxwQ"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.1' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class HeKasraCorrection:\n",
    "    def __init__(self, processed_text):\n",
    "        self.processed_text = processed_text\n",
    "        self.corrections = {\n",
    "            'correct': processed_text['raw_text'],\n",
    "        }\n",
    "        self.correction_judgements = defaultdict(list)\n",
    "\n",
    "    def vote_for_correction(self, invalid_token, corrected_token, str_index, order=10):\n",
    "        self.correction_judgements[str_index].append({\n",
    "            'invalid_token': invalid_token,\n",
    "            'corrected_token': corrected_token,\n",
    "            'str_index': str_index,\n",
    "            'order': order,\n",
    "        })\n",
    "        return self.correction_judgements[str_index]\n",
    "\n",
    "    def veto_correction(self, already_correct_token, str_index):\n",
    "        self.correction_judgements[str_index].append({\n",
    "            'invalid_token': already_correct_token,\n",
    "            'corrected_token': already_correct_token,\n",
    "            'str_index': str_index,\n",
    "            'order': 0,\n",
    "        })\n",
    "        return self.correction_judgements[str_index]\n",
    "\n",
    "    def apply_correction_judgements(self, token, str_index):\n",
    "        judgements = self.correction_judgements[str_index]\n",
    "        if len(judgements) == 0:\n",
    "            return\n",
    "\n",
    "        sorted_judgements = sorted(judgements, key=lambda x: x['order'])\n",
    "        prioritized_correction = sorted_judgements[0]\n",
    "        corrected_form = self.corrections['correct'][:str_index] + prioritized_correction['corrected_token'] + self.corrections['correct'][str_index+len(prioritized_correction['invalid_token']):]\n",
    "        self.corrections['correct'] = corrected_form\n",
    "        if token != prioritized_correction['corrected_token']:\n",
    "            self.corrections[prioritized_correction['invalid_token']] = [int(str_index), int(str_index)+len(prioritized_correction['invalid_token'])]\n",
    "\n",
    "    def finalize(self):\n",
    "        for str_index in self.correction_judgements.copy().keys():\n",
    "            self.apply_correction_judgements(self.correction_judgements['invalid_token'], str_index)\n",
    "        if self.corrections['correct'] == self.processed_text['raw_text']:\n",
    "          self.corrections = {}\n",
    "        return {\n",
    "            **self.processed_text,\n",
    "            'correction': self.corrections,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkdeTp_FDdGk"
   },
   "source": [
    "**__init__(processed_text):** This constructor initializes the instance by storing the processed text (including the raw text) and setting up a corrections dictionary with the raw text as its initial value, while also preparing a default dictionary to gather correction judgements indexed by string positions.\n",
    "\n",
    "**vote_for_correction(invalid_token, corrected_token, str_index, order=10):** This method records a suggested correction for a detected error by appending a dictionary—containing the invalid token, its proposed correction, its position, and a priority order (default 10)—to the list of judgements at the specified string index.\n",
    "\n",
    "**veto_correction(already_correct_token, str_index):** This function acts to override previous correction suggestions by appending a veto entry that marks the token as correct (using an order of 0) at the given index, ensuring that any earlier corrections for that token are effectively negated.\n",
    "\n",
    "**apply_correction_judgements(token, str_index):** This method applies the correction by first retrieving and sorting all judgements at a given string index by priority, then updating the corrected text by replacing the identified invalid token with the highest priority correction; if the token changes, it also records the error's location.\n",
    "\n",
    "**finalize():** This function iterates over all stored correction judgements to apply the prioritized corrections to the text, and if no changes are detected (i.e., the corrected text remains identical to the raw text), it clears the corrections before returning the processed text merged with any corrections made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7nUBayxdKNFe"
   },
   "outputs": [],
   "source": [
    "from dadmatools.embeddings import get_embedding\n",
    "# Some downloading, so separate the cell\n",
    "embeddings = get_embedding('word2vec-conll')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBvOzYJK2obm"
   },
   "source": [
    "#### `HeKasraExtractor` Class\n",
    "\n",
    "This class serves as the core component of the service. It takes a Persian text as input, detects Heh-Kasra errors, and returns the corrected text along with the exact spans of the identified errors. To detect Heh-Kasra mistakes, the `run` function of this class executes a pipeline that includes preprocessing the text, annotating it, and identifying various types of Heh-Kasra errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I0igwGnUd1TP",
    "outputId": "b7e74962-5a13-44e3-c175-776476419921"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-26 15:35:14--  https://github.com/sobhe/hazm/releases/download/v0.5/resources-0.5.zip\n",
      "Resolving github.com (github.com)... 20.205.243.166\n",
      "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://github.com/roshan-research/hazm/releases/download/v0.5/resources-0.5.zip [following]\n",
      "--2025-03-26 15:35:14--  https://github.com/roshan-research/hazm/releases/download/v0.5/resources-0.5.zip\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/13956112/8c6c89ce-1918-11e5-9f06-86f58ea50386?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250326%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250326T153514Z&X-Amz-Expires=300&X-Amz-Signature=9db54d931d3dd70839bcf44f6bd5c5ca0488d2331fc0b8355773a1e2f3df8df8&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dresources-0.5.zip&response-content-type=application%2Foctet-stream [following]\n",
      "--2025-03-26 15:35:14--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/13956112/8c6c89ce-1918-11e5-9f06-86f58ea50386?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250326%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250326T153514Z&X-Amz-Expires=300&X-Amz-Signature=9db54d931d3dd70839bcf44f6bd5c5ca0488d2331fc0b8355773a1e2f3df8df8&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dresources-0.5.zip&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 30557783 (29M) [application/octet-stream]\n",
      "Saving to: ‘resources-0.5.zip.1’\n",
      "\n",
      "resources-0.5.zip.1 100%[===================>]  29.14M  --.-KB/s    in 0.09s   \n",
      "\n",
      "2025-03-26 15:35:15 (321 MB/s) - ‘resources-0.5.zip.1’ saved [30557783/30557783]\n",
      "\n",
      "Archive:  resources-0.5.zip\n",
      "replace chunker.model? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "  inflating: chunker.model           \n",
      "replace langModel.mco? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "  inflating: langModel.mco           \n",
      "replace lib/liblinear-1.8.jar? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "  inflating: lib/liblinear-1.8.jar   \n",
      "replace lib/libsvm.jar? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "  inflating: lib/libsvm.jar          \n",
      "replace lib/log4j.jar? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "  inflating: lib/log4j.jar           \n",
      "replace malt.jar? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "  inflating: malt.jar                \n",
      "replace postagger.model? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "  inflating: postagger.model         \n"
     ]
    }
   ],
   "source": [
    "!wget\n",
    "!unzip resources-0.5.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PK3TWkIUGxwQ"
   },
   "outputs": [],
   "source": [
    "from hazm import WordTokenizer, POSTagger, Normalizer, InformalNormalizer, SentenceTokenizer, Lemmatizer\n",
    "import re\n",
    "\n",
    "normalizer = Normalizer()\n",
    "inf_normalizer = InformalNormalizer(seperation_flag=True)\n",
    "sent_tokenizer = SentenceTokenizer\n",
    "tokenizer = WordTokenizer(join_verb_parts=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gxtxURLHNBjI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4p19UwuOgh7O"
   },
   "outputs": [],
   "source": [
    "tagger = POSTagger(model=\"./pos_tagger.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT2ag-hYIeQf"
   },
   "source": [
    "The line `tagger = POSTagger(model=\"./postagger.model\")` creates an instance of the `POSTagger` class using a pre-trained model located at the path `./postagger.model`. This object (`tagger`) is used to assign part-of-speech (POS) tags to tokens in a text, which is essential for identifying grammatical roles (e.g., noun, verb, adjective) and is a key step in processing and analyzing linguistic structures like Heh-Kasra errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-1Z9BD-U2obt"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class HeKasraExtractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        normalized_text = normalizer.normalize(text)\n",
    "        tokens = tokenizer.tokenize(normalized_text)\n",
    "        tagged_tokens = tagger.tag(tokens)\n",
    "\n",
    "        return {\n",
    "            'raw_text': text,\n",
    "            'pipe_text': text,\n",
    "            'normalized_text': normalized_text,\n",
    "            'tokens': tokens,\n",
    "            'pos_tags': tagged_tokens\n",
    "        }\n",
    "\n",
    "\n",
    "    def vote_n_adj_he_kasra(self, he_kasra_correction, processed_text):\n",
    "        pos_pairs = zip(processed_text['pos_tags'][:-1], processed_text['pos_tags'][1:])\n",
    "\n",
    "\n",
    "        for ppair in pos_pairs:\n",
    "          p1, p2 = ppair\n",
    "          token_1, tag_1 = p1\n",
    "          token_2, tag_2 = p2\n",
    "\n",
    "          first_token_roles = ('N', 'Ne', 'PRO', 'AJ', 'AJe')\n",
    "          second_token_roles = ('N', 'Ne', 'AJ', 'PRO', 'AJe')\n",
    "\n",
    "          if tag_1 in first_token_roles and tag_2 in second_token_roles:\n",
    "              if token_1.endswith('ه'):\n",
    "                he_kasra_correction.vote_for_correction(token_1, token_1[:-1], processed_text['raw_text'].index(token_1))\n",
    "\n",
    "\n",
    "    def check_word_contains_he(self, word, next_word, text):\n",
    "        normalized_word = normalizer.normalize(word)\n",
    "\n",
    "        if not normalized_word.endswith('ه'):\n",
    "            return False\n",
    "\n",
    "        vocab = embeddings.get_vocab()\n",
    "        plural_form = word + 'ها'\n",
    "        if plural_form not in vocab:\n",
    "          return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def check_sent_has_verb(self, processed_text):\n",
    "      return 'V' in [t[1] for t in processed_text['pos_tags']]\n",
    "\n",
    "    def check_sent_has_too_many_res(self, processed_text):\n",
    "      is_res = [r[1] == 'RES' for r in processed_text['pos_tags']]\n",
    "      ratio = is_res.count(True)/len(is_res)\n",
    "      return ratio > 0.75\n",
    "\n",
    "    def check_sent_should_have_he_as_verb(self, he_kasra_correction, processed_text):\n",
    "      if self.check_sent_has_verb(processed_text):\n",
    "        return False\n",
    "      if not self.check_sent_has_too_many_res(processed_text):\n",
    "        return False\n",
    "\n",
    "      new_sent = processed_text['raw_text'] + 'ه'\n",
    "      new_pr = self.preprocess(new_sent)\n",
    "      if self.check_sent_has_verb(new_pr) and not self.check_sent_has_too_many_res(new_pr):\n",
    "        latest_token = processed_text['pos_tags'][-1][0]\n",
    "        he_kasra_correction.vote_for_correction(latest_token, latest_token + 'ه', processed_text['raw_text'].index(latest_token))\n",
    "        processed_text['pos_tags'] = new_pr['pos_tags']\n",
    "        processed_text['tokens'] = new_pr['tokens']\n",
    "\n",
    "\n",
    "    def veto_if_word_he_part_of_word(self, he_kasra_correction, processed_text):\n",
    "        pos_pairs = zip(processed_text['pos_tags'][:-1], processed_text['pos_tags'][1:])\n",
    "\n",
    "\n",
    "        for ppair in pos_pairs:\n",
    "          p1, p2 = ppair\n",
    "          token_1, tag_1 = p1\n",
    "          token_2, tag_2 = p2\n",
    "\n",
    "          contains_he = self.check_word_contains_he(token_1, token_2, processed_text)\n",
    "          if contains_he:\n",
    "              he_kasra_correction.veto_correction(token_1, processed_text['raw_text'].index(token_1))\n",
    "\n",
    "    def run(self, input_sentence):\n",
    "        prep_text = self.preprocess(input_sentence)\n",
    "        he_kasra_correction = HeKasraCorrection(prep_text)\n",
    "        pipe = [\n",
    "            self.check_sent_should_have_he_as_verb,\n",
    "            self.vote_n_adj_he_kasra,\n",
    "            self.veto_if_word_he_part_of_word,\n",
    "        ]\n",
    "\n",
    "        for func in pipe:\n",
    "            func(he_kasra_correction, prep_text)\n",
    "\n",
    "        result = he_kasra_correction.finalize()\n",
    "        return result['correction']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6C-ifuMENPa"
   },
   "source": [
    "**__init__(self):**  \n",
    "This constructor sets up an instance of the extractor without initializing any parameters, serving as a placeholder for future attributes or methods.\n",
    "\n",
    "**preprocess(self, text):**  \n",
    "This method normalizes the input text, tokenizes it, and assigns part-of-speech tags to the tokens, returning a dictionary that includes the original text, normalized text, token list, and their corresponding POS tags for further processing.\n",
    "\n",
    "**vote_n_adj_he_kasra(self, he_kasra_correction, processed_text):**  \n",
    "This function inspects adjacent token pairs in the text; if the first token (in certain noun/adjective/proper noun roles) ends with \"ه\", it votes for a correction by suggesting its removal, thereby addressing a potential Heh-Kasra error in descriptive constructions.\n",
    "\n",
    "**check_word_contains_he(self, word, next_word, text):**  \n",
    "This helper method checks if a given word ends with \"ه\" after normalization and confirms its validity by ensuring that the plural form (word+\"ها\") exists in the vocabulary, thus determining if the \"ه\" is an integral part of the word rather than an error.\n",
    "\n",
    "**check_sent_has_verb(self, processed_text):**  \n",
    "This function verifies whether the processed text contains any verb (tagged as 'V') within its POS tags, helping to decide if additional corrections, such as appending \"ه\" as a verb, are necessary.\n",
    "\n",
    "**check_sent_has_too_many_res(self, processed_text):**  \n",
    "This method calculates the ratio of tokens tagged as 'RES' in the sentence and returns True if more than 75% of the tokens fall into this category, indicating that the sentence might be over-represented by result or residual tags, affecting correction decisions.\n",
    "\n",
    "**check_sent_should_have_he_as_verb(self, he_kasra_correction, processed_text):**  \n",
    "This method determines whether the sentence is missing a verb by checking for the absence of a verb and a high ratio of 'RES' tags; it then simulates adding \"ه\" at the end, reprocesses the sentence, and if the new version shows a valid verb without excessive 'RES' tags, it votes to append \"ه\" to the last token and updates the token and POS tag lists accordingly.\n",
    "\n",
    "**veto_if_word_he_part_of_word(self, he_kasra_correction, processed_text):**  \n",
    "This function iterates over adjacent token pairs and, using the check for integral \"ه\" in a word, vetoes any correction on tokens where the \"ه\" is confirmed as an essential part of the word, ensuring that correct formations are not mistakenly altered.\n",
    "\n",
    "**run(self, input_sentence):**  \n",
    "This is the main execution method that orchestrates the entire pipeline: it preprocesses the input sentence, initializes a HeKasraCorrection object with the processed data, runs a sequence of error-checking and correction functions, finalizes the corrections, and returns the final correction results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjLiuH3i2obw"
   },
   "source": [
    "## System Performance Evaluation\n",
    "\n",
    "The code snippet below evaluates the performance of the system by creating an instance of the `HeKasraExtractor` class and testing it on a number of sample inputs. The output object of the `run` function is printed for each input in this cell’s output. As can be seen, the implemented class performs correctly for various types of Heh-Kasra errors, including those in descriptive and possessive constructions, definite marker “ه”, verb-like “ه”, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QgM5R2UYGxwR",
    "outputId": "cb32fb56-6005-4c65-c429-27569e343297"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Input: کتابه جدید\n",
      "Service Response {}\n",
      "********\n",
      "Text Input: خانه‌ی بزرگه\n",
      "Service Response {}\n",
      "********\n",
      "Text Input: دوستِ عزیزه\n",
      "Service Response {}\n",
      "********\n",
      "Text Input: این فیلمه جذابه\n",
      "Service Response {}\n",
      "********\n",
      "Text Input: سرشار از امیده\n",
      "Service Response {}\n",
      "********\n",
      "Text Input: شعرای معاصر\n",
      "Service Response {}\n",
      "********\n",
      "Text Input: ماشینهٔ جدیده\n",
      "Service Response {}\n",
      "********\n",
      "Text Input: پسرک بازیگوشه\n",
      "Service Response {}\n",
      "********\n",
      "Text Input: گل‌های رنگینه\n",
      "Service Response {}\n",
      "********\n",
      "Text Input: آبِ تمیزه\n",
      "Service Response {}\n",
      "********\n",
      "Text Input: سرورِ دلنشینه\n",
      "Service Response {}\n",
      "********\n",
      "Text Input: کتابخانه‌ی عمومی\n",
      "Service Response {}\n",
      "********\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "hkasra_extractor = HeKasraExtractor()\n",
    "input_samples = [\n",
    "    {\n",
    "      'text_input': 'کتابه جدید',\n",
    "      'expected_corrected_text': 'کتاب جدید',\n",
    "      'correct_input': False\n",
    "    },\n",
    "    {\n",
    "      'text_input': 'خانه‌ی بزرگه',\n",
    "      'expected_corrected_text': 'خانه‌ی بزرگ',\n",
    "      'correct_input': False\n",
    "    },\n",
    "    {\n",
    "      'text_input': 'دوستِ عزیزه',\n",
    "      'expected_corrected_text': 'دوست عزیز',\n",
    "      'correct_input': False\n",
    "    },\n",
    "    {\n",
    "      'text_input': 'این فیلمه جذابه',\n",
    "      'expected_corrected_text': 'این فیلم جذابه',\n",
    "      'correct_input': False\n",
    "    },\n",
    "    {\n",
    "      'text_input': 'سرشار از امیده',\n",
    "      'expected_corrected_text': 'سرشار از امید',\n",
    "      'correct_input': False\n",
    "    },\n",
    "    {\n",
    "      'text_input': 'شعرای معاصر',\n",
    "      'expected_corrected_text': 'شعرای معاصر',\n",
    "      'correct_input': True\n",
    "    },\n",
    "    {\n",
    "      'text_input': 'ماشینهٔ جدیده',\n",
    "      'expected_corrected_text': 'ماشینهٔ جدید',\n",
    "      'correct_input': False\n",
    "    },\n",
    "    {\n",
    "      'text_input': 'پسرک بازیگوشه',\n",
    "      'expected_corrected_text': 'پسرک بازیگوش',\n",
    "      'correct_input': False\n",
    "    },\n",
    "    {\n",
    "      'text_input': 'گل‌های رنگینه',\n",
    "      'expected_corrected_text': 'گل‌های رنگین',\n",
    "      'correct_input': False\n",
    "    },\n",
    "    {\n",
    "      'text_input': 'آبِ تمیزه',\n",
    "      'expected_corrected_text': 'آب تمیز',\n",
    "      'correct_input': False\n",
    "    },\n",
    "    {\n",
    "      'text_input': 'سرورِ دلنشینه',\n",
    "      'expected_corrected_text': 'سرور دلنشین',\n",
    "      'correct_input': False\n",
    "    },\n",
    "    {\n",
    "      'text_input': 'کتابخانه‌ی عمومی',\n",
    "      'expected_corrected_text': 'کتابخانه‌ی عمومی',\n",
    "      'correct_input': True\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "evaluation = np.zeros((len(input_samples), 5), dtype=object)\n",
    "for index, sample in enumerate(input_samples):\n",
    "  response = hkasra_extractor.run(sample['text_input'])\n",
    "  corrected_text = response['correct'] if 'correct' in response else sample['text_input']\n",
    "  print('Text Input: %s' % sample['text_input'])\n",
    "  print('Service Response', response)\n",
    "  print('********')\n",
    "  evaluation[index] = [sample['text_input'], sample['expected_corrected_text'], corrected_text, sample['expected_corrected_text'] == corrected_text, sample['text_input'] == sample['expected_corrected_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ND1TdfZ-g367"
   },
   "source": [
    "## Results on Test Data\n",
    "\n",
    "The following code prints a table that displays, for each input, both the expected (correct) output and the service’s output. In the next cell, the model’s accuracy in detecting the presence or absence of Heh-Kasra errors is evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 739
    },
    "id": "f_SfFLVxY-Jo",
    "outputId": "781497f5-cdb8-4e5d-a57d-6c8032f591f7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-d3637fc7-4a04-4dee-acbe-cfd0d213e365\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Raw_Input</th>\n",
       "      <th>Expected_Output</th>\n",
       "      <th>Model_Output</th>\n",
       "      <th>Correct_Prediction</th>\n",
       "      <th>No_HeKasra_Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>کوروشه کبیر</td>\n",
       "      <td>کوروش کبیر</td>\n",
       "      <td>کوروش کبیر</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>حال من خوب است.</td>\n",
       "      <td>حال من خوب است.</td>\n",
       "      <td>حال من خوب است.</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>حاله من خوبه</td>\n",
       "      <td>حال من خوبه</td>\n",
       "      <td>حال من خوبه</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>حاله من خوب</td>\n",
       "      <td>حال من خوبه</td>\n",
       "      <td>حال من خوبه</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>من اگه کتابه تو رو داشتم</td>\n",
       "      <td>من اگه کتاب تو رو داشتم</td>\n",
       "      <td>من اگه کتابه تو رو داشتم</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>پسره داشت میرفت مدرسه</td>\n",
       "      <td>پسره داشت میرفت مدرسه</td>\n",
       "      <td>پسره داشت میرفت مدرسه</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>این دختره دیوانه کار دستمون داد</td>\n",
       "      <td>این دختر دیوانه کار دستمون داد</td>\n",
       "      <td>این دختر ددیوانهکار دستمون داد</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>گل زیبا</td>\n",
       "      <td>گل زیبا</td>\n",
       "      <td>گل زیبا</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>گله زیبایی را تقدیم کردم</td>\n",
       "      <td>گل زیبایی را تقدیم کردم</td>\n",
       "      <td>گل زیبایی را تقدیم کردم</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>درختِ بزرگ</td>\n",
       "      <td>درختِ بزرگ</td>\n",
       "      <td>درختِ بزرگ</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>این کتاب خوبه</td>\n",
       "      <td>این کتاب خوبه</td>\n",
       "      <td>این کتاب خوبه</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>دستش خیلی تنده</td>\n",
       "      <td>دستش خیلی تنده</td>\n",
       "      <td>دستش خیلی تنده</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>فرشه خیلی قشنگ بود.</td>\n",
       "      <td>فرشه خیلی قشنگ بود.</td>\n",
       "      <td>فرشه خیلی قشنگ بود.</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>بسه دیگه خسته شدم.</td>\n",
       "      <td>بسه دیگه خسته شدم.</td>\n",
       "      <td>بسه دیگه خسته شدم.</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>خورشیده طلایی رنگ طلوع کرد.</td>\n",
       "      <td>خورشید طلایی رنگ طلوع کرد.</td>\n",
       "      <td>خورشید طلایی رنگ طلوع کرد.</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>یه سر به پیجِ ما بزنید</td>\n",
       "      <td>یه سر به پیجِ ما بزنید</td>\n",
       "      <td>یه سر به پیجِ ما بزنید</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>اون خیابونه رو بستن جدیداً.</td>\n",
       "      <td>اون خیابونه رو بستن جدیداً.</td>\n",
       "      <td>اون خیابونه رو بستن جدیداً.</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>علی پرروئه</td>\n",
       "      <td>علی پرروئه</td>\n",
       "      <td>علی پرروئه</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>اصلاً نمی‌فهمم از چیه من خوشش اومد!</td>\n",
       "      <td>اصلاً نمی‌فهمم از چی من خوشش اومد!</td>\n",
       "      <td>اصلاً نمی‌فهمم از چی من خوشش اومد!</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>اون اصلا غذا نمی­خورِ</td>\n",
       "      <td>اون اصلا غذا نمیخوره</td>\n",
       "      <td>اون اصلا غذا نمی­خورِ</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>برکه‌ی مشهور</td>\n",
       "      <td>برکه‌ی مشهور</td>\n",
       "      <td>برکه‌ی مشهور</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>فرشته مرگ</td>\n",
       "      <td>فرشته مرگ</td>\n",
       "      <td>فرشته مرگ</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d3637fc7-4a04-4dee-acbe-cfd0d213e365')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-d3637fc7-4a04-4dee-acbe-cfd0d213e365 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-d3637fc7-4a04-4dee-acbe-cfd0d213e365');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                              Raw_Input                     Expected_Output  \\\n",
       "0                           کوروشه کبیر                          کوروش کبیر   \n",
       "1                       حال من خوب است.                     حال من خوب است.   \n",
       "2                          حاله من خوبه                         حال من خوبه   \n",
       "3                           حاله من خوب                         حال من خوبه   \n",
       "4              من اگه کتابه تو رو داشتم             من اگه کتاب تو رو داشتم   \n",
       "5                 پسره داشت میرفت مدرسه               پسره داشت میرفت مدرسه   \n",
       "6       این دختره دیوانه کار دستمون داد      این دختر دیوانه کار دستمون داد   \n",
       "7                               گل زیبا                             گل زیبا   \n",
       "8              گله زیبایی را تقدیم کردم             گل زیبایی را تقدیم کردم   \n",
       "9                            درختِ بزرگ                          درختِ بزرگ   \n",
       "10                        این کتاب خوبه                       این کتاب خوبه   \n",
       "11                       دستش خیلی تنده                      دستش خیلی تنده   \n",
       "12                  فرشه خیلی قشنگ بود.                 فرشه خیلی قشنگ بود.   \n",
       "13                   بسه دیگه خسته شدم.                  بسه دیگه خسته شدم.   \n",
       "14          خورشیده طلایی رنگ طلوع کرد.          خورشید طلایی رنگ طلوع کرد.   \n",
       "15               یه سر به پیجِ ما بزنید              یه سر به پیجِ ما بزنید   \n",
       "16          اون خیابونه رو بستن جدیداً.         اون خیابونه رو بستن جدیداً.   \n",
       "17                           علی پرروئه                          علی پرروئه   \n",
       "18  اصلاً نمی‌فهمم از چیه من خوشش اومد!  اصلاً نمی‌فهمم از چی من خوشش اومد!   \n",
       "19                اون اصلا غذا نمی­خورِ                اون اصلا غذا نمیخوره   \n",
       "20                         برکه‌ی مشهور                        برکه‌ی مشهور   \n",
       "21                            فرشته مرگ                           فرشته مرگ   \n",
       "\n",
       "                          Model_Output Correct_Prediction No_HeKasra_Error  \n",
       "0                           کوروش کبیر               True            False  \n",
       "1                      حال من خوب است.               True             True  \n",
       "2                          حال من خوبه               True            False  \n",
       "3                          حال من خوبه               True            False  \n",
       "4             من اگه کتابه تو رو داشتم              False            False  \n",
       "5                پسره داشت میرفت مدرسه               True             True  \n",
       "6       این دختر ددیوانهکار دستمون داد              False            False  \n",
       "7                              گل زیبا               True             True  \n",
       "8              گل زیبایی را تقدیم کردم               True            False  \n",
       "9                           درختِ بزرگ               True             True  \n",
       "10                       این کتاب خوبه               True             True  \n",
       "11                      دستش خیلی تنده               True             True  \n",
       "12                 فرشه خیلی قشنگ بود.               True             True  \n",
       "13                  بسه دیگه خسته شدم.               True             True  \n",
       "14          خورشید طلایی رنگ طلوع کرد.               True            False  \n",
       "15              یه سر به پیجِ ما بزنید               True             True  \n",
       "16         اون خیابونه رو بستن جدیداً.               True             True  \n",
       "17                          علی پرروئه               True             True  \n",
       "18  اصلاً نمی‌فهمم از چی من خوشش اومد!               True            False  \n",
       "19               اون اصلا غذا نمی­خورِ              False            False  \n",
       "20                        برکه‌ی مشهور               True             True  \n",
       "21                           فرشته مرگ               True             True  "
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "evaluation_df = pd.DataFrame(evaluation, columns=['Raw_Input', 'Expected_Output', 'Model_Output', 'Correct_Prediction', 'No_HeKasra_Error'])\n",
    "evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9LGC0cMZbPGA",
    "outputId": "252616e7-e5a5-44c9-d0d5-b8a5ecdf1185"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.863636\n",
      "Model Accuracy When HeKasra Error Occured: 0.666667\n",
      "Model Accuracy When Input Was HeKasra Error Free: 1.000000\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluation_df['Correct_Prediction'].mean()\n",
    "he_kasra_acc = evaluation_df.query('No_HeKasra_Error == False')['Correct_Prediction'].mean()\n",
    "he_kasra_free_acc = evaluation_df.query('No_HeKasra_Error == True')['Correct_Prediction'].mean()\n",
    "\n",
    "print(\"Model Accuracy: %1f\" % accuracy)\n",
    "print(\"Model Accuracy When HeKasra Error Occured: %1f\" % he_kasra_acc)\n",
    "print(\"Model Accuracy When Input Was HeKasra Error Free: %1f\" % he_kasra_free_acc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
